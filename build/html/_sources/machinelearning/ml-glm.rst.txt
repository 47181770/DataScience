样例- 逻辑回归
~~~~~~~~~~~~~~~~~~~~~
 
  逻辑回归是一种非常强大和流行的分类方法，是小数据集也可以用来构造逻辑回归分类器。一旦模型建立，即使是大样本数据集的记录进行分类，计算速度也会更快成本更低便宜。
  逻辑回归扩展了线性回归的的思想。

- 逻辑回归（Logistic Regression）
  
  讨论使用系数与统计显著性来判断变量重要性，降维的变量选择算法。因为逻辑回归的因变量与自变量不是线性关系，所以不使用最小二乘法估计，而是使用最大似然估计。应用场景如下：

     1. 将客户分类为退货与不退货的类型

     2. 借贷信用评分

     3. 计量经济学中描述选择行为

     4. 二分类：如成功/失败，是/否，买/不买，存货/死亡等


  逻辑回归一般分为如下两个步骤：

     1. 对属于每个类的“倾向”或“可能”进行估计

     2. 我们使用这些概率的截止值，以便将每种情况归类到一个类


  TREE算法优缺点如下：

 ============================== ======================================================================================
        优点                                                  缺点
 ============================== ======================================================================================
  不需要转换变量                        对数据的变化敏感，细微的变化也会导致不同的分裂 
  变量子集选择是自动化的                非线性和非参数化
  对缺失值异常值的鲁棒性非常强          需要大量的数据集才能构造一个好的分类器
  泛化错误率不超贝叶斯2倍               由于多次排序、剪枝导致计算资源消耗多
 ============================== ======================================================================================


- 模型性能提升

    1. 随机森林Random Forests算法
           随机森林测量不同预测因子的相关贡献，用于对“变量重要性”评分。步骤如下：
           a. 从数据抽取多个采样样本，并进行替换（这种采样法叫bootstrap）
           b. 在每个阶段使用预测因子的随机子集，为每个样本匹配一个分类树（回归树），从而获得一个“森林”
           c. 结合各个树的预测/分类以获得改进的预测。用投票来分类，用平均来预测

    2. Boosted Trees 提升树算法
           提升树支持回归跟分类。
           a. 画一个给出错误分类记录的更高选择概率的样本
           b. 让树适应新样本
           c. 重复上述a,b步骤
           d. 使用权重投票对记录进行分类，对后期的树使用更高的权重

    3. 注意：Random Forests与Boosted Trees的使用效果较好，但是会导致模型的可解释性大大降低


- 分类树代码样例


.. code:: r 

 # 使用RidingMovers割草机的数据集
 # 1. 计算信息熵与基尼杂质指数
 > mower.df <- read.csv("D:\\Books\\RidingMowers.csv")
 > head(mower.df)
   Income Lot_Size Ownership
 1   60.0     18.4     Owner
 2   85.5     16.8     Owner
 3   64.8     21.6     Owner
 4   61.5     20.8     Owner
 5   87.0     23.6     Owner
 6  110.1     19.2     Owner
 > dim(mower.df)
 [1] 24  3
 > 
 > library(rpart)
 > library(rpart.plot)
 Error in library(rpart.plot) : 不存在叫‘rpart.plot’这个名字的程辑包
 # 下载rpart.plot.zip文件，手工安装
 # 执行rpart()运行分类树，rpart函数split分割选项默认使用基尼系数，非默认参数为(split='information')
 # 定义rpart.control决定树的深度
 > calss.tree <- rpart(Ownership ~ ., data = mower.df, control = rpart.control(maxdepth = 2), method = "class")
 # prp画出树，color,shape, information都可以显示
 > prp(class.tree3, type = 1, extra = 1, split.font = 1 , varlen = 10)
 


.. image:: _static/classification_tree.PNG
   :align: center

.. code:: r

 >
