.. _header-n2:

算法
====

机器学习常用算法数学公式

.. _header-n4:

机器学习算法
------------

.. _header-n6:

监督学习算法
~~~~~~~~~~~~

.. _header-n7:

一、朴素贝叶斯
^^^^^^^^^^^^^^

   请以\ **嫁与不嫁高富帅矮矬穷或者西瓜书**\ 的分类做理解

   假设：每对特征之间相互\ **独立**\ ，
   很多情况下，朴素贝叶斯工作情况良好，特别是文本分类、文档分类、语言主题分类、垃圾邮件过滤

给定一个类别 :math:`y` 和一个从 :math:`x_1`\ 到 :math:`x_n`
的相关的特征向量， 贝叶斯定理阐述了以下关系，方程式如下：

.. math:: P(y \mid x_1, \dots, x_n) = \frac{P(y) P(x_1, \dots x_n \mid y)}{P(x_1, \dots, x_n)}  = \frac{P(y) P(x_1  \mid y) \dots P(x_n \mid y)}{P(x_1) \dots P(x_n)}

:math:`P(y)` 是类别的“先验”概率，\ :math:`P(x \mid y)`
是特征\ :math:`x`\ 对于类别标记\ :math:`y`\ 的类条件概率，或称为“似然”(likelihood)；简化为如下：

.. math:: P(y \mid x_1, \dots, x_n) = \frac{P(y) \prod_{i=1}^{n} P(x_i \mid y)}{P(x_1, \dots, x_n)}]

在如上公式分类规则中，给定的如下分母\ :math:`P(x_1, \dots , x_n) = P(x_1) \dots P(x_n)`
的\ **分类**\ 算法中，

.. math::

   {P(x_1, \dots , x_n)} = P(x_1) \dots P(x_n) 
   \quad 对所有类标记相同

这是一个 **常量**
，在类别对比中，分母相同，所以可以直接去掉分母，只比较分子，公式\ **简化为**\ 如下贝叶斯判定规则：

.. math:: P(y \mid x_1, \dots, x_n) \propto P(y) \prod_{i=1}^{n} P(x_i \mid y)

.. math:: \Downarrow

.. math::

   \hat{y} = \arg\max_y P(y) \prod_{i=1}^{n} P(x_i \mid y),

   \quad 对于“连乘”，通常通过取对数的方式转成“连加”，避免数值下溢

朴素贝叶斯分类器的训练过程是基于训练集来估计类别先验概率\ :math:`P(y)`\ ，并为每个属性/特征\ :math:`x_i`\ 估计条件概率\ :math:`P(x_i \mid y)`

.. _header-n22:

1.1 高斯朴素贝叶斯
''''''''''''''''''

GuassianNB
实现了运用分类的高斯朴素贝叶斯算法，特征的可能性（即概率）假设为高斯分布：

.. math:: P(x_i \mid y) = \frac{1}{\sqrt{2 \pi \sigma_y^2}} exp(- \frac {(x_i - \mu_y)^2}{2 \sigma_y^2})

-  说明：对连续属性而言，高斯模型假设特征符合正态分布，根据样本计算出均值和方差，就是概率密度函数等式右边其参数为\ :math:`\mu_y 均值`\ 与\ :math:`\sigma_y 方差`

-  用途：处理连续型的特征变量，如身高<=160cm是1，160-170cm是2，>170cm是3，可以将连续变量身高转换为离散变量：3个特征f1、f2、f3，但这些方式不够细腻。故引入高斯模型，可以很好地计算男、女身高、体重、脚掌的均值与方差，这样来计算性别。

.. _header-n32:

1.2 多项式分布朴素贝叶斯
''''''''''''''''''''''''

MultinomialNB 实现了服从多项分布数据的朴素贝叶斯算法（MNB）， 与TF-IDF
同属于适用文本分类的\ **两大经典算法**

-  说明：对\ **离散**\ 属性而言，对每一个类别\ :math:`y`\ ，\ :math:`n`\ 是特征数（在文本分类中是词汇量vocabulary），样本中特征\ :math:`x_i`\ 属于类别\ :math:`y`\ 的概率\ :math:`P(x_i \mid y)`\ 可估计为：

   .. math:: P(x_i \mid y) = \frac {|D_{y,xi}|}{|D_y|}

   .. math:: \Downarrow

   为了避免其他属性携带的信息被训练集中未出现的属性值“抹去”，在估计概率值时通常要进行“平滑(smoothing)”，常用拉普拉斯平滑(Lapalce
   smoothing :math:`\alpha = 1`)或Lidstone平滑(Lidstone smoothing
   :math:`\alpha < 1`)
   ，\ :math:`N`\ 表示训练集\ :math:`D`\ 中可能的类别数，\ :math:`|D_y|`\ 是类别中所有的特征的总计数，\ :math:`|D_{y,x_1}|`\ 是特征在训练集的样本中出现的次数，公式(8)修正为如下：

   .. math:: \hat P(x_i \mid y) = \frac {|D_{y,xi}| + \alpha}{|D_y| + \alpha N}

-  用途：本算法在文本分类表现良好

.. _header-n43:

1.3 伯努利朴素贝叶斯
''''''''''''''''''''

-  说明：BernoulliNB实现了用于多重伯努利分布数据的朴素贝叶斯训练和分类算法，即有多个特征，但每个特征都假设是一个二元变量(Bernoulli，Boolean)，要求样本特征以二元值特征向量表示：

   .. math:: P(x_i \mid y) = P(i \mid y) x_i + (1- P(i \mid y))(1 - x_i)

-  用途：适合在短文本分类

.. _header-n50:

1.4 补充朴素贝叶斯
''''''''''''''''''

-  说明：ComplementNB
   实现了补充朴素贝叶斯（CNB）算法，是1.2的（MNB）的一种改进算法，CNB使用每个类的补集的统计量来计算模型的权重，CNB的参数估计比MNB的参数估计更稳定，比MNB表现的更好

-  用途：特别适用于不平衡数据集，解决了MMB中较长文档主导参数估计的趋势。

.. _header-n56:

二、线性回归模型
^^^^^^^^^^^^^^^^

目标值 :math:`y` 是输入变量 :math:`x` 的线性组合。定义向量
:math:` (w_1, \dots , w_p)`\ 代表系数coef\_，\ :math:`w_0`
代表截距intercept\_，数学概念表示为：

.. math:: \hat y (w,x) = w_0 + w_1 x_1 + \dots + w_p x_p

-  说明：算法是拟合一个带有系数\ :math:`w = (w_1, \cdots, w_p)`\ 的线性模型，是的数据集实际\ **观测数据**\ 与\ **预测数据**\ 之间的残差平方和最小，数学概念表示为：

   .. math:: \underset{w}{min\,} \mid \mid X_w - y \mid \mid^2

-  用途：各特征\ :math:`x_1, \cdots, x_p`\ 相互独立的情况下预测连续变量。

-  

.. _header-n68:

三、线性判别分析与二次线性判别分析
^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^

-  说明

-  用途

.. _header-n74:

四、支持向量机
^^^^^^^^^^^^^^

-  说明

-  用途

.. _header-n80:

五、k近邻KNN
^^^^^^^^^^^^

-  参考：[KNN最近邻算法]:https://theroadtodatascience.readthedocs.io/en/latest/machinelearning/ml-knn.html

.. _header-n84:

六、决策树
^^^^^^^^^^

-  

.. _header-n89:

七、集成方法
^^^^^^^^^^^^

-  说明

-  用途

.. _header-n96:

八、TF-IDF（单文本词频-逆文档频率）
^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^

-  说明：TF：Term Frequency - IDF：Inverse Document Frequency
   搜索关键词权重的科学度量、是对搜索关键词的重要性的度量，具备很强的理论依据

-  用途：TF-IDF是信息检索中最重要的发明，在搜索、文献分类和其他领域有广泛的应用

-  度量网页和查询的相关性的原理：

   1. 如果一个查询（贝叶斯算法的用途）包含\ :math:`N`\ 个关键词\ :math:`w_1,w2,\dots,w_n`\ ，他们在一个特定网页中的词频分别是：\ :math:`TF_1,TF_2,\dots,TF_N `\ ，那么这个查询的单文本词频就是每个\ *分词*\ 后的词语的和：\ :math:`TF_1 + TF_2 + \dots + TF_N`\ 。比如某网页上一共1000个词，“贝叶斯”、“算法”、“的”、“用途”分别出现了3次、10次、20次、5次，那么他们的词频分别是：0.003、0.01、0.02、0.005，其和
      :math:`0.003 + 0.01 + 0.02 + 0.005 = 0.038`
      就是“贝叶斯算法的用途”的“单文本词频”

   2. 上面词语中"的"是停用词，权重为零

   3. 假定关键词\ :math:`w`\ 在\ :math:`D_w`\ 个网页中出现过，那么\ :math:`D_w`\ 越大，w的权重就越小，\ :math:`D_w`\ 越小，w权重越大，如上面“的”在每个网页都有，所以权重就最小为0。\ **逆文本频率IDF**\ 公式为\ :math:`log (\frac {D} {D_w})`\ ，其中\ :math:`D`\ 是总网页数目，\ :math:`D_w`\ 是出现关键词的网页数目

   4. 短语相关性的计算公式就由词频简单求和变成了加权求和，公式如下

      .. math:: TF_1 \cdot IDF_1 + TF_2 \cdot IDF_2 + \cdots + TF_N \cdot IDF_N

   5. 共有10亿个网页\ :math:`D=10亿`\ ，如“的”字，在10亿个网页中都出现过，就是\ :math:`D_w=10亿`\ ，所以“的”字权重就是\ :math:`log( \frac {D}{D_w})=log( \frac {D=10亿}{D_w=十亿}) = log(1) = 0`\ ，如“贝叶斯”在100万个网页中出现，“贝叶斯”的权重就是\ :math:`log( \frac {D}{D_w}) = log(\frac {D=10亿}{D_w=100万})=log(1000)=9.966`\ ；如“算法”在250万个网页中出现，“算法”的权重就是\ :math:`log( \frac {D}{D_w}) = log(\frac {D=10亿}{D_w=250万})=log(400)=8.6438`\ ；如“用途”在500万个网页中出现，“用途”的权重就是\ :math:`log( \frac {D}{D_w}) = log(\frac {D=10亿}{D_w=500万})=log(200)=7.6438`\ ；“的”为停用词，权重为0。

      .. code:: python

         import math
         print(math.log(1000,2))
         print(math.log(400,2))
         print(math.log(200,2))

   6. "贝叶斯算法的用途" 短语的TF-IDF详细计算结果如下：

      .. math:: 0.003 \cdot 9.966 + 0.01 \cdot 8.6438 + 0.02 \cdot 0 + 0.005 \cdot 7.6438 = 0.154555

   7. 结合网页排名（PageRank）算法，给定一个查询，搜索有关网页的综合排名大致由\ **相关性和网页排名的乘积**\ 决定。

.. _header-n123:

非监督学习算法
~~~~~~~~~~~~~~

.. _header-n125:

一、高斯混合模型
^^^^^^^^^^^^^^^^

-  说明

-  用途

.. _header-n131:

二、聚类
^^^^^^^^

-  说明

-  用途

.. _header-n140:

Python/R算法
------------
