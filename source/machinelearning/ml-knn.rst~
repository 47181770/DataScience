样例- k-NN k近邻
~~~~~~~~~~~~~~~~~~~~~


- k-NN（k-Nearest Neighbors k近邻）
  
  k近邻算法是一种常用的监督学习算法，主要用于分类也可用于回归，是一个理论上成熟，应用最简单的机器学习算法之一。没有显式的训练过程，训练时间开销为零，是“懒惰学习 Lazy learning”的注明代表。应用领域在计算机视觉、推荐系统（如歌曲、影视等），数据缺失值预处理亦可。
  k近邻工作机制：

     1. 给定测试样本，基于某种距离度量（如欧氏距离等），计算已知数据集中的点与当前点的距离

     2. 按距离递增次序排序，找出训练集中与其最靠近的k个训练样本

     3. 基于这k个“邻居”的信息，选取与当前数据点最近的k个点（简而言之：给一个新的数据时，离它最近的 k 个点中，哪个类别多，这个数据就属于哪一类）

     4. “投票法”：选择k个样本中出现最多的类别标记作为预测结果

     5. “平均法”：回归任务中，将这k个样本的实值输出标记的平均值作为预测结果

     6. “加权平均”或“加权投票”：基于距离远近进行，距离越近，样本权重越大

  kNN算法优缺点如下：

 ============================= ======================================================================================
        优点                                                  缺点
 ============================= ======================================================================================
  简单且有效                           在发现特征之间关系上的能力有限，类别评分不像概率评分
  对数据的分布没有要求                 分类阶段慢
  训练阶段很快（训练与重训）           新记录预测，全部重计算复杂度高、空间复杂度高，需大量的内存，如何实时预测？
  泛化错误率不超贝叶斯2倍              样本不平衡时，预测偏差大
 ============================= ======================================================================================

 
 
.. code:: r


 # 利用数据集进行kNN计算
